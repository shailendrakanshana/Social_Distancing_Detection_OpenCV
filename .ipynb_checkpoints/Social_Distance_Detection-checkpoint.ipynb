{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3227d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418ddcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#setting up variable for intializing minimum to filter weak detections\n",
    "min_config = 0.3\n",
    "nms_thres = 0.3\n",
    "\n",
    "\n",
    "#defining the minimum distance(in pixels) between two people\n",
    "min_dist = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275bbff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating  a function for detection of the people\n",
    "def detect_people(frame,net,ln,personId=0):\n",
    "    #grabing the dimensions of the frame all intializing the new list \n",
    "    (H,W) = frame.shape[:2]\n",
    "    results = []\n",
    "    # construct the blob from the input frames and perfrom the forward pass of YOLO object-detection\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame,1/255.0,(416,416),swapRB = True,crop = False)\n",
    "    net.setInput(blob)\n",
    "    layerOutput = net.forward(ln)\n",
    "    \n",
    "    #creating our list of centorids,bound boxes and confidence\n",
    "    \n",
    "    boxes = []\n",
    "    centorids = []\n",
    "    confidences = []\n",
    "    \n",
    "    #loop over each of layer ouput\n",
    "    for output in layerOutput:\n",
    "        #loop over each of the detection\n",
    "        for detection in output:\n",
    "            # extraction classId and confidence of current object detection\n",
    "            scores = detection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            confidence = scores[classId]\n",
    "            \n",
    "            #filter detection by ensuring that the object detected was a person and that the minimum confidence \n",
    "            if classId == personId and confidence<min_config:\n",
    "                #scale the bounding box coordinates back to the size of the image that YOLO return (x,y) coordinates followed \n",
    "                #by width and hight of the boxes\n",
    "                box = detection[0:4]*np.array([W,H,W,H])\n",
    "                (centerX,centerY,width,hight) = box.astype(\"int\")\n",
    "                \n",
    "                #using the center(x,y) to find the top and left corner of the box\n",
    "                \n",
    "                x = (int)(centerX-(width/2))\n",
    "                y = (int)(centerY-(hight/2))\n",
    "                \n",
    "                #updating the list of the boxes,centorids and confidence\n",
    "                \n",
    "                boxes.append([x,y,int(width),int(hight)])\n",
    "                centorids.append((centerX,centerY))\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    # apply non-suppression to suppress weak detection\n",
    "\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes,confidences,min_config,nms_thres)\n",
    "    \n",
    "    #ensure that at least one person detect\n",
    "    \n",
    "    if len(idxs)>0:\n",
    "        \n",
    "        for i in idxs.flatten():\n",
    "            #extracting the box coordinates\n",
    "            (x,y) = (boxes[i][0],boxes[i][1])\n",
    "            (w,h) = (boxes[i][2],boxes[i][3])\n",
    "            #update results list consist of person prediction, box coordinates and centorids\n",
    "            \n",
    "            r = (confidences[i],x,y,x+w,y+h,centorids[i])\n",
    "            results.append(r)\n",
    "            \n",
    "    return results\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88018320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO for Social Distance Detection.......\n",
      "Accessing the Video......\n"
     ]
    }
   ],
   "source": [
    "#importing necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "import argparse\n",
    "import imutils\n",
    "import os\n",
    "from configs import config\n",
    "\n",
    "\n",
    "#construct the agrument parse and parse the arguments\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\n",
    "\thelp=\"path to (optional) input video file\")\n",
    "ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\n",
    "\thelp=\"path to (optional) output video file\")\n",
    "ap.add_argument(\"-d\", \"--display\", type=int, default=1,\n",
    "\thelp=\"whether or not output frame should be displayed\")\n",
    "ap.add_argument('-f')\n",
    "args = vars(ap.parse_args([\"--input\",\"input2.mp4\",\"--output\",\"output.mp4\",\"--display\",\"1\"]))\n",
    "#Load the coco class labels the YOLO model\n",
    "\n",
    "labelsPath = os.path.sep.join([\"coco.names.txt\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "#drive the paths to the yolo weights and model configuration\n",
    "\n",
    "weigthsPath = os.path.sep.join([\"yolov3.weights\"])\n",
    "cofigPath = os.path.sep.join([\"yolov3.cfg\"])\n",
    "\n",
    "#load yolo object detection trained\n",
    "\n",
    "print(\"Loading YOLO for Social Distance Detection.......\")\n",
    "net = cv2.dnn.readNetFromDarknet(cofigPath,weigthsPath)\n",
    "\n",
    "#determine only the \"output\" layer names that we need from YOLO\n",
    "\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "#Initialize the video stream and pointer to ouput video\n",
    "\n",
    "print(\"Accessing the Video......\")\n",
    "vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\n",
    "writer = None\n",
    "\n",
    "#loop over frame from the video \n",
    "\n",
    "while True:\n",
    "    #reading the next frame from file\n",
    "    (gradded,frame) = vs.read()\n",
    "    \n",
    "    #if the frame was not gradded, then reached to the end to the stream\n",
    "    if not gradded:\n",
    "        break\n",
    "    \n",
    "    #resize the frame and then detect people in the video\n",
    "    \n",
    "    frame = imutils.resize(frame,width = 700)\n",
    "    results = detect_people(frame,net,ln,personId=LABELS.index(\"person\"))\n",
    "    \n",
    "    #initialize the set of indexex that violate the minimum distance\n",
    "    \n",
    "    violate = set()\n",
    "    \n",
    "    #ensure there are at least two people for detections\n",
    "    \n",
    "    if len(results)>=2:\n",
    "        #extracting all centroids from the results and computing the euclidean distances\n",
    "        \n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        D = dist.cdist(centroids,centroids,metric = \"euclidean\")\n",
    "        \n",
    "        #loop to add the centroid who violating the social distance\n",
    "        \n",
    "        for i in range(0,D.shape[0]):\n",
    "            for j in range(i+1,D.shape[1]):\n",
    "                #check if the distance between two centroid pair is less than minimum distance\n",
    "                if D[i,j] < min_dist:\n",
    "                    #update our violation set with index of the centorid\n",
    "                    violate.add(i)\n",
    "                    violate.add(j)\n",
    "    \n",
    "    # loop over the results list\n",
    "    for (i ,(prob,bbox,centroid)) in enumerate(results):\n",
    "        #extracting bounding box and centroids coordinates\n",
    "        (startX,startY,endX,endY) = bbox\n",
    "        (cX,cY) = centroid\n",
    "        color = (0,255,0)\n",
    "        \n",
    "        #if the index pair exists within violation set then,update the color\n",
    "        \n",
    "        if i in violate:\n",
    "            color = (0,0,255)\n",
    "            \n",
    "    #drawing a bounding box around the perosn and the coordiantes of the person\n",
    "        \n",
    "        cv2.rectangle(frame,(startX,startY),(endX,endY),color,2)\n",
    "        cv2.circle(frame,(cX,cY),5,color,1)\n",
    "        \n",
    "    #drawing the total number of the violations on the ouput \n",
    "        \n",
    "    text = \"Social Distancing Violations:{}\".format(len(violate))\n",
    "    cv2.putText(frame,text,(10,frame.shape[0]-25),cv2.FONT_HERSHEY_SIMPLEX,0.85,(0,0,255),3)\n",
    "        \n",
    "    #check to see if the output frame should be displayed \n",
    "    \n",
    "    if args[\"display\"]>0:\n",
    "        cv2.imshow(\"Frame\",frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        #if 'q' key pressed, break the loop\n",
    "        \n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "    \n",
    "    # if an ouput video file path has been supplied and the video writer has not been initialized\n",
    "    if args[\"output\"] !=\"\" and writer is None:\n",
    "        #initialize our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(args[\"output\"], fourcc, 25,(frame.shape[1], frame.shape[0]), True)\n",
    "    \n",
    "    \n",
    "    #if video writer is not None, write the frame to the Ouput\n",
    "    \n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        \n",
    "        \n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fd675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
